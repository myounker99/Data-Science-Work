{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5\n",
    "\n",
    "## Due November 18th @ 11:55 PM\n",
    "\n",
    "I'm really passionate about having you build your own implementations of exisiting algorithms.  In order to understand cars I still remember helping my friend rebuild an engine, and while I'm definitely not a \"car person\" it was an enlightening expierience nonetheless!  \n",
    "\n",
    "I thought about having a \"use it\" assignment - but I realize I can save that for labs.  \n",
    "\n",
    "For this homework you'll have your **choice** between building two algorithms!  (You will not be responsible for the algorithm you choose not to build).  The two algorithms under discussion in this assignment are **Random Forest Classifier** and **Naive Bayes**.  The first question I will probably get is: \"Why can't we build a decision tree from scratch?\".  My guess is many of you can but it is incredibly challenging and recursive in nature (recursion seems to be a somewhat \"unfun\" topic for students).  Therefore - Random Forest is the better way to go here.\n",
    "\n",
    "**Random Forest instructions below:** \n",
    "\n",
    "**You many not import a Random Forest classifier!** \n",
    "\n",
    "While a Random Forest is indeed made of decision trees, I will not be making you build decision trees in this random forest, you may simply use the sklearn implementation of the DecsionTreeClassifier and then use those trees to build your Random Forest!  You will not setup a test train split on the data provided (it's loan approval data split into a train/test file).  Fit a Decision Tree model using the training data. Let the tree grow fully. Use the model to predict labels for both training data and test data. Report training accuracy and test accuracy.  Next use cross validation and loop over maximum depths from two to ten inclusive (rebuild the tree each time).  Keep track of the optimal depth that maximizes accuracy.\n",
    "\n",
    "Time to build your model!  The following are necessary parameters (name them as I did): \n",
    "\n",
    "1) - num_estimator, Number of estimators (this is the number of trees in your Random Forest)\n",
    "\n",
    "2) - pct_ft, Percentage of features you want each tree to use, recall if we have a dataset of 10 features (columns) and each tree in the random forest takes on 5 features at random, you will quite likely have distinct trees.  Just think briefly for a moment, with 10 features choosing 5 features at random, how many trees can you build?  If I were to choose feature 5,4,8,2,1 and feature 8,2,1,5,4 would this yield the same tree or not?  \n",
    "\n",
    "3) - max_depth, Maximum allowable depth default to letting the Random Forest fully grow but you can limit the growth this param.\n",
    "\n",
    "You will have a .fit and a .predict like with KNN.  Your code should handle pandas/numpy input (you might choose to standardize it in your model but it needs to be flexible enough to accept either input)\n",
    "\n",
    "You will cross validate and vary the \"depth\" you let your tree get too as before (2 to 10 inclusive).  Recall that a tree with N features can grow to N levels deep.  We might be overfitting our data without \"pruning\" our tree. (in fact decision trees can easily have 100% training accuracy provided no label noise occurs).  Compare the optimal depth that maximizes accuracy and compare it against your result from your decision tree.  What do you notice?\n",
    "\n",
    "**Naive Bayes instructions below**\n",
    "\n",
    "**You may not import Naive Bayes classifier!**\n",
    "\n",
    "For Naive Bayes - you may use the count vectorizer (recall the lab) to store the counts of words, you will find a dataset (other than Ham/Spam for texts/emails) and do light preprocessing (recall the lab) to be able to store words. Make a train/test split of 75%/25%.  You will then make your model, and make predictions (no cross validation) on this one.  This assignment is arguably slightly less work, but less guidance has been provided (to balance each choice out).  Please provide a link where you got your data from (so we don't need to pass around data).  I highly suggest twitter data of two or more tweeters where you can predict on an unkown tweet which tweeter said it.  You can handle the inputted data however you choose, just make sure you are fitting and predicting with your model in your submitted notebook (so I can see how it works from a cursory glance before I get into the code)\n",
    "\n",
    "**Extra credit:**\n",
    "\n",
    "I very much enjoy Game Theory and this is a perfect way to apply it to this class!\n",
    "\n",
    "(26 students are currently enrolled in this course)\n",
    "\n",
    "I love both of these algorithms so if it were up to me, I'd ask half the class to do one while the other half the class did the other.  Since you have a choice obviously that may not happen, but I will incentivize the 50/50 split.\n",
    "\n",
    "If I receive a 13/13 split - everyone gets 10 points of extra credit on their assignment.\n",
    "\n",
    "If I receive a 14/12 split - everyone gets 8 points of extra credit on their assignment.\n",
    "\n",
    "If I receive a 15/11 split - everyone gets 6 points of extra credit on their assignment.\n",
    "\n",
    "If I receive a 16/10 split - everyone gets 4 points of extra credit on their assignment.\n",
    "\n",
    "If I receive a 17/9 split - everyone gets 2 points of extra credit on their assignment.\n",
    "\n",
    "If I get 18 or more of the same assignment, no penalty just no extra credit.  You do not need to lock in your choice, you merely turn in the assignment so nobody will know until the assignments are graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify tweet as biden or trump\n",
    "# trump: https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv\n",
    "# biden: https://www.kaggle.com/rohanrao/joe-biden-tweets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#of the timestamp col, the first 4 values are always the year. I only want the year from the col, so only grab that\n",
    "def get_year(str_arg):\n",
    "    return int(str_arg[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the naive bayes lab\n",
    "def clean_string_short(str_arg,lowercase=False):\n",
    "    str_arg=re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+',\"\",str_arg) #Get rid of URL links\n",
    "    clean=re.sub('[^a-z\\s]+','',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    clean=re.sub('(\\s+)',' ',clean) #multiple spaces now single space\n",
    "    #converting the cleaned string to lower case (this is maybe not best \n",
    "    #idea for our data)\n",
    "    words = clean.split()\n",
    "    lc = [i for i in words if len(i) <= 10]\n",
    "    return \" \".join(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13950, 2)\n",
      "(13484, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tweeter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28078</th>\n",
       "      <td>I started my business with very little and bui...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27209</th>\n",
       "      <td>Does anybody remember when Bill Clinton in wor...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36909</th>\n",
       "      <td>Border Patrol and Law has captured large numbe...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40296</th>\n",
       "      <td>Neither one of the Democrat star witnesses at ...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31254</th>\n",
       "      <td>Join me at pm over at the Lincoln Memorial wit...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37427</th>\n",
       "      <td>I am continuing to monitor the situation in Ve...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35212</th>\n",
       "      <td>The only thing James Comey ever got right was ...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27394</th>\n",
       "      <td>Ted Cruz is falling in the polls He is nervous...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34771</th>\n",
       "      <td>No Collusion No but that doesnt matter because...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>and got me wrong right from the beginning and ...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content tweeter\n",
       "28078  I started my business with very little and bui...   Trump\n",
       "27209  Does anybody remember when Bill Clinton in wor...   Trump\n",
       "36909  Border Patrol and Law has captured large numbe...   Trump\n",
       "40296  Neither one of the Democrat star witnesses at ...   Trump\n",
       "31254  Join me at pm over at the Lincoln Memorial wit...   Trump\n",
       "...                                                  ...     ...\n",
       "37427  I am continuing to monitor the situation in Ve...   Trump\n",
       "35212  The only thing James Comey ever got right was ...   Trump\n",
       "27394  Ted Cruz is falling in the polls He is nervous...   Trump\n",
       "34771  No Collusion No but that doesnt matter because...   Trump\n",
       "31306  and got me wrong right from the beginning and ...   Trump\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_raw = pd.read_csv(\"tweets_trump.csv\")\n",
    "\n",
    "#only tweets from 2016+\n",
    "trump_raw[\"date\"] = trump_raw[\"date\"].apply(lambda x: get_year(x))\n",
    "    \n",
    "t_filtered = trump_raw[trump_raw[\"date\"] >= 2016]\n",
    "\n",
    "#drop cols that are not needed\n",
    "t_filtered = t_filtered.drop(\"id\", 1)\n",
    "t_filtered = t_filtered.drop(\"link\", 1)\n",
    "t_filtered = t_filtered.drop(\"date\", 1)\n",
    "t_filtered = t_filtered.drop(\"favorites\", 1)\n",
    "t_filtered = t_filtered.drop(\"retweets\", 1)\n",
    "t_filtered = t_filtered.drop(\"mentions\", 1)\n",
    "t_filtered = t_filtered.drop(\"hashtags\", 1)\n",
    "t_filtered = t_filtered.drop(\"geo\", 1)\n",
    "\n",
    "#tweeter col\n",
    "l = [\"Trump\"] * len(t_filtered)\n",
    "t_filtered[\"tweeter\"] = l\n",
    "\n",
    "print(t_filtered.shape)\n",
    "\n",
    "#clean data\n",
    "t_filtered[\"content\"] = t_filtered[\"content\"].apply(lambda x: clean_string_short(x))\n",
    "#drop empty rows\n",
    "t_filtered = t_filtered[t_filtered[\"content\"] != \"\"]\n",
    "t_filtered = t_filtered[t_filtered[\"content\"] != \" \"]\n",
    "\n",
    "print(t_filtered.shape)\n",
    "\n",
    "#so that both Trump and Biden are equally represented (in the raw data, Trump far outweighs Biden in # of rows)\n",
    "t_filtered = t_filtered.sample(5000)\n",
    "t_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5161, 2)\n",
      "(5148, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tweeter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>There is no between mental health and physical...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>Last night on the debate stage I shared my bol...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Were live in a few minutes to talk about how w...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>Under President Trumps policies a was separate...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>In this moment were facing both a public healt...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>TeamJoe is hosting house parties across the co...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>LGBT rights are human rights Prejudice is prej...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>The United States should be leading on this is...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>Climate change is the crisis of our time On da...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>There is nothing moderate about what President...</td>\n",
       "      <td>Biden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content tweeter\n",
       "2216  There is no between mental health and physical...   Biden\n",
       "1936  Last night on the debate stage I shared my bol...   Biden\n",
       "1069  Were live in a few minutes to talk about how w...   Biden\n",
       "1569  Under President Trumps policies a was separate...   Biden\n",
       "5431  In this moment were facing both a public healt...   Biden\n",
       "...                                                 ...     ...\n",
       "1407  TeamJoe is hosting house parties across the co...   Biden\n",
       "1044  LGBT rights are human rights Prejudice is prej...   Biden\n",
       "1188  The United States should be leading on this is...   Biden\n",
       "3702  Climate change is the crisis of our time On da...   Biden\n",
       "1943  There is nothing moderate about what President...   Biden\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_raw = pd.read_csv(\"tweets_biden.csv\")\n",
    "\n",
    "#only tweets from 2016+\n",
    "biden_raw[\"timestamp\"] = biden_raw[\"timestamp\"].apply(lambda x: get_year(x))\n",
    "    \n",
    "b_filtered = biden_raw[biden_raw[\"timestamp\"] >= 2016]    \n",
    "\n",
    "#drop cols that are not needed\n",
    "b_filtered = b_filtered.drop(\"id\", 1)\n",
    "b_filtered = b_filtered.drop(\"timestamp\", 1)\n",
    "b_filtered = b_filtered.drop(\"url\", 1)\n",
    "b_filtered = b_filtered.drop(\"replies\", 1)\n",
    "b_filtered = b_filtered.drop(\"retweets\", 1)\n",
    "b_filtered = b_filtered.drop(\"quotes\", 1)\n",
    "b_filtered = b_filtered.drop(\"likes\", 1)\n",
    "\n",
    "#so make col names the same between tables so pd can concat them\n",
    "b_filtered.rename(columns = {\"tweet\": \"content\"}, inplace = True)\n",
    "\n",
    "#tweeter col\n",
    "l = [\"Biden\"] * len(b_filtered)\n",
    "b_filtered[\"tweeter\"] = l\n",
    "\n",
    "print(b_filtered.shape)\n",
    "#clean data\n",
    "b_filtered[\"content\"] = b_filtered[\"content\"].apply(lambda x: clean_string_short(x))\n",
    "#drop empty rows\n",
    "b_filtered = b_filtered[b_filtered[\"content\"] != \"\"]\n",
    "b_filtered = b_filtered[b_filtered[\"content\"] != \" \"]\n",
    "\n",
    "print(b_filtered.shape)\n",
    "b_filtered = b_filtered.sample(5000)\n",
    "b_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data, then clean the content of garbage\n",
    "df = pd.concat([t_filtered, b_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data, count vectorizer\n",
    "df = df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,) (2500,) (7500,) (7500,)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "x_train,x_test,y_train,y_test = train_test_split(df[\"content\"],df[\"tweeter\"],test_size=.25, stratify=df[\"tweeter\"])\n",
    "\n",
    "print(x_test.shape, y_test.shape, x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nb_360:\n",
    "    \n",
    "    def sep_by_class_(self, x):\n",
    "        seperated = {}\n",
    "        \n",
    "        #seperate rows by class\n",
    "        for i,row in x.iterrows():\n",
    "            if row[\"class\"] not in seperated:\n",
    "                seperated[row[\"class\"]] = []\n",
    "            seperated[row[\"class\"]].append(row)            \n",
    "            \n",
    "        #index of this class corresponds to the class names\n",
    "        self.classes = list()\n",
    "        for item in seperated:\n",
    "            self.classes.append(item)\n",
    "        \n",
    "        #return a list of dataframes for each class/index\n",
    "        result = list()\n",
    "        for c in self.classes:\n",
    "            result.append(pd.DataFrame(seperated[c]))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def create_cvs_(self, dfs_list):\n",
    "        result = list()    \n",
    "                    \n",
    "        #create a count vectorizer for each df\n",
    "        for df in dfs_list:\n",
    "            cv = CountVectorizer()\n",
    "            cv.fit(df[\"content\"])\n",
    "            count_arr = cv.transform(df[\"content\"]).toarray()\n",
    "            \n",
    "            #for each df, row=tweet, col=word, and individual cell is the count of that word in the tweet\n",
    "            result.append(pd.DataFrame(count_arr, columns=cv.get_feature_names()))\n",
    "                        \n",
    "        #returns a list of dfs\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def fit(self, inputted_x_data, inputted_y_data):\n",
    "        #take in data\n",
    "        self.x_data = inputted_x_data\n",
    "        self.y_data = inputted_y_data\n",
    "        \n",
    "        #combine data in order to train\n",
    "        temp_data = self.x_data\n",
    "        temp_data[\"class\"] = self.y_data\n",
    "        \n",
    "        #use the combined data to get a list of dataframes (1 df for each class)\n",
    "        self.class_dfs_list = self.sep_by_class_(temp_data)\n",
    "        \n",
    "        #using count vectorizer, \n",
    "        self.final_dfs = self.create_cvs_(self.class_dfs_list)\n",
    "        \n",
    "        #gets the count of total words (not unique) for each df\n",
    "        total_words_per_class = list()\n",
    "        for i in range(len(self.final_dfs)):\n",
    "            total_words_per_class.append(self.final_dfs[i].to_numpy().sum())\n",
    "        \n",
    "        #gets total words (not unique) for ALL dfs in order to get ratio\n",
    "        self.total_words = 0\n",
    "        for n in total_words_per_class:\n",
    "            self.total_words += n\n",
    "        \n",
    "        #probablity that a tweet came from class at index i\n",
    "        self.prob_per_class = list()\n",
    "        denom = self.total_words\n",
    "        for i in range(len(total_words_per_class)):\n",
    "            self.prob_per_class.append(total_words_per_class[i]/self.total_words)\n",
    "        \n",
    "    def prob_of_word_(self, word, class_i):\n",
    "        #laplace smoothing, so no probability ends up being 0\n",
    "        alpha = 1\n",
    "        \n",
    "        #only attempt further operations if the word exists in the existing df, otherwise utilize alpha to make value nonzero\n",
    "        if word in self.final_dfs[class_i]:\n",
    "            l = self.final_dfs[class_i][word].value_counts().to_dict()\n",
    "        else:\n",
    "            return (alpha)/(self.total_words+alpha)\n",
    "        \n",
    "        #number of times the word occured in the given df\n",
    "        p_of_word = 0\n",
    "        for key,value in l.items():\n",
    "            p_of_word += (key*value)\n",
    "        \n",
    "        #return the number of instances / total words, end up with porportion of word in df\n",
    "        p_of_word = (p_of_word+alpha)/(self.total_words+alpha)\n",
    "        \n",
    "        return p_of_word\n",
    "        \n",
    "    #given a list of words, determine the class\n",
    "    def classify_(self, tweet):\n",
    "        prob_of_class_list = list()\n",
    "        \n",
    "        #start each class operation with the P(class)\n",
    "        for i in range(len(self.classes)):\n",
    "            prob_of_class_list.append(self.prob_per_class[i])\n",
    "                \n",
    "        #for each class,\n",
    "        for i in range(len(self.classes)):\n",
    "            #get probability of each word and multiply it into the final prediction value\n",
    "            for word in tweet:\n",
    "                prob_of_class_list[i] = prob_of_class_list[i] * self.prob_of_word_(word,i)\n",
    "        \n",
    "        #return most likely tweeter, aka the larget prediction value\n",
    "        return self.classes[prob_of_class_list.index(max(prob_of_class_list))]\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        result_list = list()\n",
    "        #for each row in the df\n",
    "        for i,row in test_data.iterrows():\n",
    "            #row to string, cut off the unecessary data, then split into each individual word\n",
    "            tweet = row.to_string()[11:].split(\" \")\n",
    "            #append the prediction\n",
    "            result_list.append(self.classify_(tweet))\n",
    "        \n",
    "        #returns a list of each prediction\n",
    "        return (result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = nb_360()\n",
    "nb.fit(x_train.to_frame(), y_train.to_frame())\n",
    "pred = nb.predict(x_test.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.62x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'y_Predicted': pred,\n",
    "        'y_Actual':    y_test\n",
    "        }\n",
    "\n",
    "df_conf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_conf['y_Actual'], df_conf['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "print(sn.heatmap(confusion_matrix, annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work decently, but it way overpredicts in Biden's favor. I imagine this has to do with the P(class) value through pretty much every calculation in favor of Biden (his was ~.56, while Trump's was ~.44). There were also a decent number of garbage words generated by the cleaning (but still an improvement over unfiltered data). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
